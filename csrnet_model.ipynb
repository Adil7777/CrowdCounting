{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CSRNet(nn.Module):\n",
    "    def __init__(self, load_pretrained=True):\n",
    "        super(CSRNet, self).__init__()\n",
    "\n",
    "        vgg = models.vgg16(pretrained=load_pretrained)\n",
    "        self.frontend = nn.Sequential(*list(vgg.features.children())[:23])  # Up to conv4_3\n",
    "\n",
    "        self.backend = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, 3, padding=2, dilation=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, 3, padding=2, dilation=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, 3, padding=2, dilation=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 256, 3, padding=2, dilation=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 128, 3, padding=2, dilation=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 64, 3, padding=2, dilation=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.output_layer = nn.Conv2d(64, 1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.frontend(x)\n",
    "        x = self.backend(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrowdDataset(Dataset):\n",
    "    def __init__(self, image_folder, density_map_folder, image_files, transform=None):\n",
    "        self.image_folder = image_folder\n",
    "        self.density_map_folder = density_map_folder\n",
    "        self.image_files = image_files # Ensure sorted list\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        img_path = self.image_files[idx]\n",
    "        # print(img_path)\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "        image = image.astype(np.float32) / 255.0  # Normalize to [0,1]\n",
    "\n",
    "        # Load density map (.npy file)\n",
    "        density_map_name = self.image_files[idx].replace('.jpg', '_density.npy').replace(self.image_folder, self.density_map_folder)  # or '.png' -> '.npy'\n",
    "        density_map = np.load(density_map_name)  # Loads (H, W) float32 density map\n",
    "\n",
    "        # Convert to torch tensors\n",
    "        image = torch.from_numpy(image).permute(2, 0, 1)  # (C, H, W)\n",
    "        density_map = torch.from_numpy(density_map).unsqueeze(0).float()  # (1, H, W)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, density_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_resized_folder = \"Train_resized_2\"\n",
    "train_density_folder = \"Train_density_2\"\n",
    "\n",
    "train_image_paths = []\n",
    "train_heatmap_paths = []\n",
    "\n",
    "# Iterate over files in Train_resized folder\n",
    "for file_name in os.listdir(train_resized_folder):\n",
    "    if file_name.endswith(\".jpg\"):\n",
    "        train_image_paths.append(os.path.join(train_resized_folder, file_name))\n",
    "\n",
    "# Iterate over files in Train_density folder\n",
    "for file_name in os.listdir(train_density_folder):\n",
    "    if file_name.endswith(\".npy\"):\n",
    "        train_heatmap_paths.append(os.path.join(train_density_folder, file_name))\n",
    "\n",
    "print(f\"Found {len(train_image_paths)} images and {len(train_heatmap_paths)} heatmaps.\")\n",
    "\n",
    "\n",
    "validation_resized_folder = \"Validation_resized_2\"\n",
    "validation_density_folder = \"Validation_density_2\"\n",
    "validation_image_paths = []\n",
    "validation_heatmap_paths = []\n",
    "\n",
    "# Iterate over files in Validation_resized folder\n",
    "for file_name in os.listdir(validation_resized_folder):\n",
    "    if file_name.endswith(\".jpg\"):\n",
    "        validation_image_paths.append(os.path.join(validation_resized_folder, file_name))\n",
    "\n",
    "# Iterate over files in Validation_density folder\n",
    "for file_name in os.listdir(validation_density_folder):\n",
    "    if file_name.endswith(\".npy\"):\n",
    "        validation_heatmap_paths.append(os.path.join(validation_density_folder, file_name))\n",
    "\n",
    "print(f\"Found {len(validation_image_paths)} images and {len(validation_heatmap_paths)} heatmaps.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CrowdDataset(train_resized_folder, train_density_folder, train_image_paths)\n",
    "validation_dataset = CrowdDataset(validation_resized_folder, validation_density_folder, validation_image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(model, train_loader, val_loader=None, num_epochs=10, lr=1e-5, device='cuda'):\n",
    "    model = model.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # TRAINING\n",
    "        for images, targets in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            images = images.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "\n",
    "        avg_train_loss = running_loss / len(train_loader.dataset)\n",
    "        print(f\"\\nEpoch {epoch+1} - Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        # VALIDATION\n",
    "        if val_loader is not None:\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for images, targets in val_loader:\n",
    "                    images = images.to(device)\n",
    "                    targets = targets.to(device)\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, targets)\n",
    "                    val_loss += loss.item() * images.size(0)\n",
    "\n",
    "            avg_val_loss = val_loss / len(val_loader.dataset)\n",
    "            print(f\"Epoch {epoch+1} - Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    return model\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
